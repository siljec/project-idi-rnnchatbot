\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}


\title{Project Task: Chatbot}


\author{
 \\
\texttt{} \\
\AND
Silje Christensen \\
NTNU \\
\texttt{siljechristensen92@gmail.com} \\
\And
Simen Johnsrud \\
NTNU \\
\texttt{simen.johnsrud92@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
TBA\\
Second version.
\end{abstract}

\section{Introduction}
Conversational agents have been a hot topic of research the past years. With the increasing amount of large conversational data available, we can feed a chatbot with more knowledge and get smarter systems. Several papers has explored how the best versions of certain models can be implemented, yet we are still not there.

Our goal is to make the foundation to make a useful chatbot. We will study the current state of art of RNN chatbots. Further, we will compare a Sequence to Sequence architecture using the LSTM cells with the same architecture using Grid-LSTM cells, proposed by (Kalchbrenner et al. 2016), using the Ubuntu Dialogue Corpus \cite{Ubuntuv2}. 

We will start with our motivation for this project. Then we will explain the Recurrent Neural Network, and emphasize why we think it is the best tool for making a chatbot. A lot of research has been done on language modelling, and we will study the current state of art of models used for chatbots and for other similar fields. Further, we will take a look at the different sequence modelling approaches which gives , starting at the basic explanation of how it works, to newer architectures presented in \cite{Kalchbrenner}. We investigate different datasets which could be a potential fit for a chatbot, and further study the Ubuntu Dialogue Corpus \cite{Ubuntuv2} and explain why we chose it** and how we preprocess the data. In section 8 we will explain the difficulties around the evaluation of a chatbot, and how we chose to evaluate our models. Further, in section XX we describe our baseline model. before we go in detail about our Grid-LSTM model in the following section. We will describe how we want to work during next semester in "The Road Ahead" section, and finally we sum up what we have explored. 

\todo[inline]{Write more about the next sections** }


\section{Motivation}
Even though conversational agents is a hot topic of research, you do not encounter many intelligent and useful chatbots in your everyday life. We are just not there yet. But imagine all the possibilities a general and intelligent chatbot could give us. We believe that exploring new models used for chatbots and improve the current state of art will make the foundation for new innovative solutions that we, in the future, will take for granted. 

SIMEN skriver her

\todo[inline]{Write more**}

\section{RNN}
Language modelling is a challenging task, and is a hot topic of research. Research has taught us that a RNN can learn to model a language, which we give several examples of in section 4 about the current state of art. 

A vanilla RNN implementation is a neural network that uses its own calculations as inputs. The network is more complex than an ordinary neural network, but the architecture is pretty much the same. For training purposes, the most common way is to use Stochastic Gradient Descent (SGD), but since the parameters are shared by all time steps in the network we need to change how we calculate the gradients for SGD. Back-propagation Through Time (BPTT) is one of the popular solutions. By doing this, the network is able to have some conditions between the input, which could be beneficial in the means of conversations. However, a vanilla RNN is unable to learn dependencies between words that are several steps away, not to mention repliques away. With this as a guideline, Long Short-Term Memory (LSTM) has gained better results as this kind of cells are able to create dependencies between words further away from each other then the original RNN. Vanishing gradients have also been a big problem, and a LSTM architecture is one improvement to this.

\section{State of the Art}
Different papers of how we can design a better conversational agent has been proposed during the recent years. The authors have different perspective and thoughts when it comes to modelling the best conversational agent. We differ between two main categories of conversational system models: retrieval based and generative based. The first architecture uses a repository of predefined responses and the latter will generate new sentences, which it may not have encountered before. There are pros and cons with both approaches, and both (Shang et al. 2015) and (Lowe et al.) describes why one model is better than the other with different point of view. 

Retrieval based methods often rely on manual effort in designing rules or  automatic training of the model, and they do not necessarily need a huge amount of data. The major drawback of this model is that it can only respond with sentences it has already seen, which makes it difficult to develop an expandable open domain conversational system. These facts makes (Shang et al. 2015) critical to the model. (Shang et al. 2015) embraces the increasing amount of public conversational data, and focuses on a generative approach using an encoder-decoder architecture, also known as a Sequence to Sequence architecture. This model is commonly used to solve translation problems (Sutskever et al. 2014, Cho et al. 2014, Bahdanau et al. 2016). Instead of feeding the model with an input and output as English and French, we rather feed it with both replies and  responses in English. By digging into the encoder-decoder approach, we can see that the model consists of two different RNNs. An encoder that reads and encodes source sentences into fixed-length vectors and a decoder that outputs a translation from the encoded vector (Sutskever et al. 2014, Cho et al. 2014). (Bahdanau et al. 2016) proposed a new version of the earlier described encoder-decoder model to solve the translation problem. The architecture consists of a bidirectional RNN (BiRNN) as an encoder and a decoder that emulates searching through a source sentence during decoding a translation. In their proposed scheme, they want the annotation of each word to summarize not only the preceding words, but also the following words. A BiRNN consists of forward and backward RNNs. The forward RNN reads the input sequence as it is ordered from the first word to the last word, and calculated a sequence of forward hidden states. The backward RNN reads the sequence in the reverse order, resulting in a sequence of backward hidden states. The results show that their architecture outperforms the conventional encoder-decoder model significantly, regardless of the sentence length and that it is much more robust to the length of a source sentence.

It is important to note that compared to the chatbot challenge, the translation problem is significantly easier to solve, and easier to work with as it has well defined evaluation methods, e.g. BLEU. (Shang et al. 2015) propose a Neural Responding Machine (NRM) and employs a neural encoder-decoder to solve Short Text Conversation (STC). The decoder is a standard RNN language model except that it is conditioned on the context input c (combination of the hidden representation and the attention signal), using GRU-cells. They consider three types of encoding schemes, a global scheme, a local scheme and a hybrid scheme which combines the two. The global scheme uses the final hidden state as the global representation of the sentence. The local scheme uses an attention mechanism that allows the decoder to dynamically select and linearly combine different parts of the input sequence. Both schemes has its pros and cons, and by combining them to a hybrid scheme, they get better results. 

In 2016 (Kalchbrenner et al. 2016) introduced Grid Long Short-Term Memory which can have $N$ dimensions. This $N$-dimensional network consists of LSTM cells arranged in a multidimensional grid that can be applied to several data structures, including sequences and higher dimensional data such as images. They did several experiments with their proposed model, and observed the advantages of their architecture compared to the regular LSTM network. One of the experiments used a **** two 2-dimensional grid of 3-LSTM blocks, to translate from Chinese to English, with great success. The processing is bidirectional, as one dimension processes the source sentence whereas the other dimension produces the target sentence. Unlike the simple LSTM architecture, the proposed Grid LSTM network will repeatedly scan the source sentence on each generated word. Another feature is that the source words and target words are projected on two different sides of the Grid LSTM, ensuring that the vectors will interact closely without being conflated. We will describe the Grid LSTM in detail in section XX. 

There is an obvious advantage to the generative model, but there are several reasons for why the current proposed generative models are not good enough for conversational agents, and though it has decent results for short text conversations, it is not good enough for longer sentences. The grammar may be wrong, and the structure of the sentence may not make sense at all. These problems are solved with the retrieval based model, as the sentences is picked from a fixed set, without grammar errors. Several papers has looked towards this approach (Lowe et al. 2016 +++). They consider a TF-IDF (term frequency-inverse document frequency) approach, as well as an RNN and a LSTM approach. The RNN version consists of one encoder RNN, used to encode the given context, and another RNN which generates a response by using beam-search. The major difference between (Lowe et al. 2016) and the other mentioned papers is that they are concerned with classification of responses, instead of generation. In addition to the RNN model, they consider the same architecture but changed the hidden units to LSTM units in order to model longer-term dependencies. Their results shows that the LSTM model is significantly better than pure RNN and TF-IDF evaluating with Recall@k.


\section{Background for Grid-LSTM/Arhcitecture}
\todo[inline]{We described RNN in section XX. In this section we will take a look at the .. Write more**}

We described a basic Recurrent Neural Network in section 3, now we want to take a closer look at the RNN architecture and different cells we can use. 

\subsection{Sequence to sequence model}
\includegraphics[scale=0.5]{seq2seq} \newline
\textit{Figure x: Sequence to Sequence, from Tensorflow \cite{seq2seq}. Each box in the picture above represents a cell of the RNN } \newline

We mentioned in the section about RNN that a network can learn to model a language. In this section we describe the Sequence to Sequence model, which can generate a meaningful based on some given input. The sequence to sequence model is often referred to as an Encoder-Decoder architecture, as it consists of two RNNs: an encoder that processes the input and a decoder that generates the output. Encoder and decoder can share weights or use a different set of parameters

In the following sub sections we will describe the most common cells used for Sequence to Sequence modelling, and further look at extensions of these cell. 

\subsection{LSTM}
\includegraphics[scale=0.5]{lstm_cells} \newline
\textit{Figure 1: LSTM cells. \todo{Change figure so that it is easy to see the forget gate etc..} } \newline

Long short-term memory (LSTM) is an RNN architecture first proposed in 1997 \cite{LSTM} which has been increasingly popular after its origin. The LSTMs are able to learn long term dependencies in a greater extent than before. The layer in each repeating module in the RNN is replaced with a LSTM cell and this is where the magic happens. The most important in the LSTM cell is the state. This is like a conveyor belt where the history on it may or may not be affected by new data fed into the module. The change of the state is dependent of the three gates the cell is built up of. First, we have the \textit{forget gate} which decides how much of the old history in the state that should be carried on. All of the gates consists of a sigmoid function outputs values between zero and one. If the forget gate outputs a value of one, it indicates that the all of the history should go through, while a value of zero means that nothing should pass. The second gate, the input gate, decides how much of the new stuff, the input, that should be added to the state. Values close to one means that all of the input should be added. Finally the output gate tells how much of the state that we should output.

\subsection{GRU}
The resent years, Gated Recurrent Units (GRU), have been very popular as an alternative to the LSTM. The cell consists of only two gates, \textit{reset} and \textit{update} gate. By reducing the number of gates, the model becomes easier to train and hence we get a performance gain, as well as it is easier to implement. Since it first use in 2004 \cite{Cho} it has shown good results, though it is performing slightly worse than LSTM, but the reduce in complexity has made it to a popular choice. However, the architecture is still new and not as good researched as LSTMs, which may result in discovering new drawbacks during the next years.

\subsection{Stacked LSTM}
As the popularity of the LSTMs increased, different setups of the cell and the network were further researched. Stacked LSTM is simply a network consisting of several LSTM layers. \cite{Kalchbrenner}. This means that the output from one layer of a LSTM cell will go directly into another layer of LSTM cell.

\includegraphics[scale=0.4]{stacked_vs_2dgrid} \newline
\textit{Figure 2**: Stacked LSTM vs 2D-Grid LSTM. Each block consists of several LSTM cells (Shaded rectangles). The 2D-Grid LSTM has LSTM cells along the depth dimension too, i.e. the layers are also connected. \todo{**Confusing figure, make a new one}}

\subsection{Grid LSTM}
The Grid LSTM is an extension of the LSTM cell, and a network (Kalchbrenner et al) \cite{Kalchbrenner} proposed in 2016**(version3)**. It is simply a network of LSTM cells arranged in a multidimensional grid. The network differs from existing deep LSTM architectures in that the depth dimension is treated like the other dimensions. Meaning that the cells are connected between the layers, and not just within the layers. \todo{ What is the goal.**}

One dimensional Grid LSTM is analogous to a feed-forward network that uses LSTM cells. Two dimensional Grid LSTM corresponds to the Stacked LSTM (A network with multiple LSTM layers), but it adds cells along the depth dimension too. If we have three or more dimensions, we have a Multidimensional LSTM, but it differs from this model by having the cells along the depth dimension, as well as we have a N-way interaction that is not prone to the instability present in Multi-Dimensional LSTM.**


\subsubsection{Blocks}
\includegraphics[scale=0.4]{gridblocks} \newline
\textit{Figure 3**: The structure of different blocks}

 When we talk about the Grid LSTM, we say that the architecture consists of several blocks. The structure of the blocks for different networks are illustrated in figure XX\todo{**Create this}. The standard LSTM block has one LSTM cell, i.e. it has a self loop and it can output the hidden state. The 1D Grid LSTM does not have any self loops, as it is only a cell in a feed-forward network and it is not recurrent. The 2D Grid LSTM block consists of two LSTM cells, one operates in the vertical dimension** (as in an ordinary LSTM) and the other cell (operates in the horizontal dimension) or (operates in the depth, connecting the different layers)**. The 3D Grid LSTM operates in one additional dimension.** 


\includegraphics[scale=0.5]{3dgridlstm} \newline
\subsubsection{Grid LSTM Architecture for Sequence}
 Because we study a sequence, the 2D-Grid LSTM is of our interest. The model used for the translation problem in \cite{Kalchbrenner} uses one dimension to process the source sentence whereas the other dimension produces the target sentence. The network will repeatedly scan the source sentence for each generated target word. Two stacked two-dimensional grids, operating in the opposite directions, is used to increase capacity and help with learning longer scale translation patterns by giving us a bidirectional processing. The resulting model is a three-dimensional Grid LSTM where hierarchy grows along the third dimension (the depth of the network). The block in a Grid LSTM receives N hidden vectors $h_1, ..., h_N$ as well as N memory vectors $m'_1, ..., m'_N$ as input, just as for multidimensional LSTM where the the block consists of N dimensions. The block outputs, however, N hidden vectors and N memory vectors that are all very distinct. The block will then compute one transform LSTM for each dimension. \todo{**Write more}

%-----

%Grid LSTM is a network of LSTM cells arranged in a grid of one or more dimensions \cite{Kalchbrenner}. This is analogues to the stacked LSTM, just with the possibility to add cells along the depth dimension too. A grid LSTM with three or more dimensions, on the other hand, equals a multidimensional LSTM, but with the possibility of having cells not just along the depth dimension.

%The grid LSTM have proven better results than ordinary LSTMs in the context of translation of languages \cite{Kalchbrenner}. Keeping this in mind, there is a reason to believe that grid LSTM will do good in the use of a chatbot.

%Skal mest sannsynlig fjernes
%\section{How to combine retrieval and generative models}
%The difference between the retrieval and generative models are obvious, as we state in section 4 in "The Current State of Art". The first will pick a response from a fixed set, and the latter will generate new and possible unseen responses. There are pros and cons with both models. The retrieval model is not flexible enough to be used for an expandable open domain conversational system. The generative model may have grammatically or linguistic errors, and the structure of the sentence may not make sense at all. There have not been proposed any papers on how we can extract the "best from both worlds". 

%In this phase of the project, we could implement both a generative and a retrieval based model, and compare the outputs with a scoring algorithm and show the best answer to the user. However, there are a lot of challenges due to this. First of all, for each incoming sequence, we will need to traverse two different neural networks and then run a comparable algorithm which will be time consuming. The training time will be doubled, and how should we create a scoring metrics to decide whether the retrieval one or the generative one is the best?

%An other interesting approach could be to somehow use the retrieval one to build the sentence structure, and then further fill in the words from the generative model. This could remove a lot of the grammatical errors, but would need the model to understand which part of speech each word belongs to. The challenges around this is probably beyond our knowledge.

%--------------------


\section{Exploring datasets}
During the last years, a lot of qualitative, as well as quantitative, datasets have been made available online. However, to find the perfect dataset for a chatbot is still a challenge. Conversations are often private and hence a seldom resource in order to train a chatbot. Opensubtitles[KILDE], reddit comments[KILDE] and twitter comments[KILDE] have all been used, but has the significant drawback of high noise. The conversations are often just replies or short input to a problem. Chatbots would do their best utility if they were able to answer questions from the user, and hence we would need a dataset that could learn the agent to give meaningful responses.

We are aiming for a closed-domain chatbot, that could serve as a customer support employee which means that we should struggle to find old customer support datasets or a FAQ based resource. The Ubuntu Dialogue Corpus, which is a massive set of 1 million multi-turn dialogues \cite{Lowe}, will be a good start for us.  %(DET ER PAPERET SOM HAR GJORT DETTE NB NB) In addition to the Ubuntu Dialogue Corpus, and even more qualitatively, we might be able to use a private customer support dataset given by Telenor. (MER HER NÅR VI VET HVA SOM SKJER)

\section{Ubuntu Dialogue Corpus}
The Ubuntu Dialogue Corpus was used in the paper by (Lowe et al.). The dataset contains almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words, making it a good fit for a conversational system. It is a closed domain dataset, where the data is extracted from he Ubuntu chat logs, used to receive technical support for various Ubuntu-related problems. \cite{Lowe}

\includegraphics[scale=0.5]{tsv_file_example} \newline

We downloaded the dataset using the script provided from \cite{Ubuntuv2}. The script will download all the .tsv (tab separated values) files, consisting of the exact time for when the message was sent, username of the person who sent the message, username of the receiver and the message itself. The preprocessing script which came with \cite{Ubuntuv2} will preprocess the data to be a good fit for a retrieval based model, where it will structure the data with a context, the true response, several random responses, and labels indicating if a response was correct for the context. We decided to preprocess the .tsv files from scratch, to get the data in our desired format.

We want one x\_ file, and one y\_ file, where the content in line 1 in the y\_ file is the response to line 1 in the x\_ file, and so on. The data we get from \cite{Ubuntuv2} consists of several folders consisting of .tsv files. Before we start the preprocessing, we read all the folder names, and then shuffles them. We do this because the downloaded files are sorted chronological and we want the training, validation and test sets to be randomly distributed. 

The first step in making these files, is to read all the .tsv files and strip each line to only consist of the actual message. Further we need to keep track of the conversation initiator, as we always want to fill the x\_ file with the his/hers messages. At this point, we will have two separate files where all the problems should be in the x\_ file and all of the helping responses in y\_ file. Quite often, the same user sends several messages in a row. In order to satisfy our format, it is important to merge this line into one, so that line $a$ in x\_ corresponds to line $a$ in y\_. We separate the different sentences with an $\_EOS\_$ (end of sentence) token.

All data comes with noise, in a greater or lesser extent. Some of the noise is hard to remove, but others such as typos and abbreviations is something we can get rid of and hence, improve the quality of the dataset. We used the Normalizer's \cite{Normalizer_spellfix} list of words that is often misspelled to correct typos. In addition we supplemented the file to contain typical abbreviations such as $I'am$-$I am$, and other typos such as $pasword$-$password$ that we observed in the dataset we chose. We ran through all the words, and replaced a misspelled word or an abbreviated word to the correct one.

After tokenizing our words and replacing misspelled words, we need to create a vocabulary containing the $n$ most common words based on the files we have generated so far. We enumerate all the words, and create a file consisting of the $n$ most common words, with one word at each line, sorted by the decreasing number of occurrences of each word. Further we will encode the x\_train and y\_train files, so that a word will be replaced with the index for that word in the vocabulary file. If we encounter a word that is not in the vocabulary, we use the id for the $UNK$ token. The reason for this is to improve both learning time and the result as knowing more than $n$ words most likely does not make our bot neither more understandable nor useful.

Finally, we use $10\%$ of the training files as validation set, and  $10\%$ as the test set . 

\subsection{Preprocessed data}
We have a large dataset, it took ~19 minutes to just read all the .tsv files and extract the messages on a 2,6 GHz Intel Core i5 processor. It took in total ~27 minutes to preprocess the data, and to make the training, validation and test sets. 

We read $1852868$ files, and when we are done preprocessing, we have a X\_train and Y\_train file with $4305920$**** turns, a X\_val and Y\_val with $XX$ turns, and a test set with $XX$ turns.

We have in total XXXX** sentences, from XX unique users.

\section{Evaluation}
Perplexity
Humans (us typing same sentneces to the different models - må finne passende setninger fra testdata (?) )

\section{Baseline}
For the baseline, we implemented a sequence to sequence model as described in (Sutskever et. al, Vinyals et. al) with some modifications. This was straight forward using Tensorflow's translation model, described in detail in section 10.1**. The model described in Sutskever and Vinyals is the same, but they use different number of layers and neurons, as well as they use different datasets(dobbeltsjekk). We will justify our network details in section 10.2.

\subsection{Tensorflow's Translation Model}

A basic sequence-to-sequence model, as introduced in (Cho et al. 2014), consists of two RNNs: an encoder that processes the input and a decoder that generates the output. Both the encoder and the decoder will consist of RNN cells, most commonly LSTM or GRU cells. To train the model it requires a set of inputs and a set of outputs as integer tensors, where each value is an integer indicating a word, as we described in section [UBUNTU].


\subsection{Deciding network details}
We studied the paper by Vinyals and Sutskever. Vinyals used the same model for different domains, and they also had different network details. For the closed domain "IT helpdesk" they used a single layer LSTM with 1024 memory cells using stochastic gradient descent with gradient clipping. Their vocabulary consisted of the most common 20K words, which included special tokens indicating turn taking and actor. In their other experiment, they had a two-layered LSTM with 4096 memory cells for the OpenSubtitles dataset \cite{OpenSubtitles} having the most frequent 100k words as vocabulary. \cite{Vinyals}. (Sutskever et al) made an English-French translation model, using 4 layers, with 1000 cells at each layer, with an input vocabulary of 160,000 and an output vocabulary of 80,000 \cite{Sutskever}

We decided to use two layers with 4096 memory cells, because we think that the OpenSubtitles dataset has more in common with the Ubuntu Dialogue Corpus. 

\subsection{Results}
TBA when we have trained it

\section{Experiment: Using Grid LSTM}

\subsection{Approach}
We build upon the baseline model, and will try to change the cells to GridLSTM cells. 

\subsection{Edit the seq2seq model}
GridLSTM needs a multidimensional input, and the sequence2sequence used in Vinyals was based on one dimensional LSTM-cells. The error we got when we ran the code was that the input had shape (?, number\_of\_neurons). We needed an integer, and tried with batch\_size, and it worked. When decoding we need to read sentence by sentence, hence the batch\_size is 1. We therefore passed an extra argument to the model class, as we need to know whether we are training the model or need the model for decoding. 

\subsection{Final model}

\subsection{Results}

Time for reading data: 20 min, 5sek.

Time for deleting files, reading data, spell checking, creating vocabulary (everything): 25min, 30sek


\section{Challenges}
\begin{itemize}
    \item \textbf{Noisy dataset}. Even though the Ubuntu Dialogue Corpus in categorized as a closed domain dataset,
    \item \textbf{Time constraints}. So little time, so much to dooo
    \item \textbf{Waiting for the right training environment}. It is too time consuming to train on our Macs, where we cannot run Tensorflow on our Mac's GPU. We did not have access to GPU machines with enough disk space until mid November.
    \item ++
\end{itemize}

\section{The road ahead}
TBA

\section{Conclusion}
TBA
\section{Acknowledgment}
TBA
\begin{thebibliography}{1}
\bibitem{Graves}
Graves A., Fernandez S., Schmidhuber J. Multi-Dimensional Recurrent Neural Networks, https://arxiv.org/abs/0705.2011, v1 2007
\bibitem{Bahdanau}
Bahdanau D, Cho K, Bengio Y, Neural Machine Translation by Jointly Learning to Align and Translate,  https://arxiv.org/abs/1409.0473, v7 2016
\bibitem{Dodge}
Dodge J, Gane A, Zhang X, Bordes A, Chopra S, H.Miller A, Szlam A, Weston J.
Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems. http://arxiv.org/pdf/1511.06931v6.pdf, v6 2016
\bibitem{Lowe}
Lowe R, Pow N, V. Serban I, Pineau J. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. https://arxiv.org/abs/1511.06931, v3 2016
\bibitem{Shang}
Shang L, Lu Z, Li H. Neural RespondingMachine for Short-Text Conversation. https://arxiv.org/abs/1503.02364, v2 2015
\bibitem{Vinyals}
Vinyals O, V. Le Q. A Neural Conversational Model. http://arxiv.org/abs/1506.05869, v3 2015
\bibitem{Cho}
Cho K, Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078, v3 2014
\bibitem{Sutskever}
Sutskever I, Vinyals O, Le Q, Sequence to Sequence Learning with Neural Networks. https://arxiv.org/abs/1409.3215, v3 2014
\bibitem{LSTM}
Hockreiter S, Schimhuber J, Long Short-Term Memory. http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97\_lstm.pdf, 1997
\bibitem{Kalchbrenner}
Kalchbrenner N, Danihelka I, Graves A, Grid Long Term Memory.   https://arxiv.org/abs/1507.01526, v3 2016

\bibitem{seq2seq}
Sequence to Sequence model by Tensorflow.   https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html\#sequence-to-sequence-models
\bibitem{translationModel}
Translation model by Tensorflow. https://www.tensorflow.org/versions/r0.11/tutorials/seq2seq/index.html#neural-translation-model

\bibitem{Ubuntuv2}
Dataset: Ubuntu Dialogue Corpus v2, https://github.com/rkadlec/ubuntu-ranking-dataset-creator
\bibitem{Ubuntuv1}
Dataset: Ubuntu Dialogue Corpus v1, http://cs.mcgill.ca/~jpineau/datasets/ubuntu-corpus-1.0/
\bibitem{OpenSubtitles}
Dataset: OpenSubtitles, http://opus.lingfil.uu.se/OpenSubtitles.php
\bibitem{Normalizer_spellfix}
Preprocessing with the list of common misspellings. https://github.com/superscriptjs/normalizer/blob/master/data/spellfix.txt
\end{thebibliography}
\end{document}

